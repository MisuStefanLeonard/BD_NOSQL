{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3790bbd0",
   "metadata": {},
   "source": [
    "# T4 – Caching and Data Acceleration (Redis + MongoDB)\n",
    "\n",
    "## Project Overview\n",
    "This project implements a high-performance caching layer using **Redis** to accelerate data retrieval from a **MongoDB** database (`sample_mflix` dataset).\n",
    "\n",
    "###  Objectives Met\n",
    "1.  **Architecture:** Hybrid storage using MongoDB (Persistent) and Redis (In-Memory).\n",
    "2.  **Strategies:** Implemented **Cache-Aside** (Read) and **Write-Through** (Update).\n",
    "3.  **Data Structures:** * **Hashes:** For storing movie objects.\n",
    "    * **Sorted Sets:** For a \"Trending Movies\" leaderboard.\n",
    "    * **Strings:** For system status/config.\n",
    "4.  **Performance:** Benchmarking latency and Hit/Miss ratios.\n",
    "\n",
    "###  Tech Stack\n",
    "* **Database:** MongoDB Atlas (Cloud)\n",
    "* **Cache:** FakeRedis (In-Memory simulation for portability)\n",
    "* **Language:** Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "925fa65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.15.3)\n",
      "Requirement already satisfied: fakeredis in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.33.0)\n",
      "Requirement already satisfied: redis in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (7.1.0)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: dnspython in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.2.1)\n",
      "Requirement already satisfied: sortedcontainers>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from fakeredis) (2.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymongo fakeredis redis pandas dnspython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abc1d5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Connected to MongoDB! Total Movies available: 21349\n",
      " Connected to Redis Cache!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import fakeredis\n",
    "import redis\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MONGO_URI = \"mongodb+srv://misustefan30:mocococo30@tema.z4qxsyu.mongodb.net/\"\n",
    "\n",
    "try:\n",
    "    mongo_client = MongoClient(MONGO_URI)\n",
    "    db = mongo_client.sample_mflix\n",
    "    movies_col = db.movies\n",
    "    \n",
    "    # Quick connectivity test\n",
    "    count = movies_col.count_documents({})\n",
    "    print(f\" Connected to MongoDB! Total Movies available: {count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" MongoDB Connection Error: {e}\")\n",
    "\n",
    "# 2. Connect to Redis (Simulated)\n",
    "# decode_responses=True ensures we get Strings back, not Bytes\n",
    "r = fakeredis.FakeStrictRedis(decode_responses=True)\n",
    "print(\" Connected to Redis Cache!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fddea0c",
   "metadata": {},
   "source": [
    "## 2. Architecture & Data Flow\n",
    "\n",
    "We implement the **Cache-Aside** pattern for reading data. This ensures that the application only hits the slow database (MongoDB) when absolutely necessary.\n",
    " Data Flow Diagram\n",
    "```mermaid\n",
    "graph TD\n",
    "    User[User / API Request] --> App[Application Layer]\n",
    "    \n",
    "    subgraph \"Caching Layer\"\n",
    "    App -- 1. GET 'movie:TheGodfather' --> Redis{Check Redis}\n",
    "    Redis -- HIT (Return Data) --> App\n",
    "    end\n",
    "    \n",
    "    subgraph \"Persistent Layer\"\n",
    "    Redis -- MISS (Null) --> Mongo[Query MongoDB Atlas]\n",
    "    Mongo -- Return JSON --> App\n",
    "    end\n",
    "    \n",
    "    App -- 2. SET 'movie:TheGodfather' (TTL 60s) --> Redis\n",
    "    App -- Return Response --> User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc92872a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Caching Logic Implemented.\n"
     ]
    }
   ],
   "source": [
    "# --- HELPER: Fetch from DB (The \"Slow\" Path) ---\n",
    "def fetch_from_mongo(title):\n",
    "    \"\"\"Simulates fetching from the persistent storage.\"\"\"\n",
    "    # We fetch specific fields to keep the cache object lightweight\n",
    "    movie = movies_col.find_one(\n",
    "        {\"title\": title},\n",
    "        {\"title\": 1, \"year\": 1, \"plot\": 1, \"imdb.rating\": 1, \"_id\": 0}\n",
    "    )\n",
    "    \n",
    "    if movie:\n",
    "        # Flatten structure for Redis Hash compatibility\n",
    "        # Redis Hashes are best for flat Key-Value pairs, not nested JSON\n",
    "        return {\n",
    "            \"title\": movie.get(\"title\"),\n",
    "            \"year\": str(movie.get(\"year\", \"N/A\")),\n",
    "            \"plot\": movie.get(\"plot\", \"No plot available.\"),\n",
    "            \"rating\": str(movie.get(\"imdb\", {}).get(\"rating\", \"N/A\"))\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# --- STRATEGY 1: CACHE-ASIDE (Read Operation) ---\n",
    "def get_movie(title):\n",
    "    \"\"\"\n",
    "    1. Check Redis (Fast).\n",
    "    2. If missing, Check Mongo (Slow).\n",
    "    3. Update Redis with new data + TTL.\n",
    "    \"\"\"\n",
    "    cache_key = f\"movie:{title}\"\n",
    "    \n",
    "    # A. Check Cache\n",
    "    cached_data = r.hgetall(cache_key)\n",
    "    \n",
    "    if cached_data:\n",
    "        # [Requirement: Hit]\n",
    "        return cached_data, \"HIT\"\n",
    "    \n",
    "    # B. Check Database\n",
    "    db_data = fetch_from_mongo(title)\n",
    "    \n",
    "    if db_data:\n",
    "        # [Requirement: Miss & Populate]\n",
    "        # Use Redis Hash (HSET) for object storage\n",
    "        r.hset(cache_key, mapping=db_data)\n",
    "        \n",
    "        # [Requirement: TTL / Eviction]\n",
    "        # Set data to expire after 60 seconds to prevent stale data\n",
    "        r.expire(cache_key, 60)\n",
    "        \n",
    "        return db_data, \"MISS\"\n",
    "    \n",
    "    return None, \"NOT FOUND\"\n",
    "\n",
    "# --- STRATEGY 2: WRITE-THROUGH (Update Operation) ---\n",
    "def update_movie_rating(title, new_rating):\n",
    "    \"\"\"\n",
    "    Updates the database AND the cache immediately.\n",
    "    This ensures users never see old data after an update.\n",
    "    \"\"\"\n",
    "    print(f\" Updating rating for '{title}' to {new_rating}...\")\n",
    "    \n",
    "    # 1. Update Persistent DB (MongoDB)\n",
    "    movies_col.update_one(\n",
    "        {\"title\": title}, \n",
    "        {\"$set\": {\"imdb.rating\": new_rating}}\n",
    "    )\n",
    "    \n",
    "    # 2. Update Cache (Redis)\n",
    "    cache_key = f\"movie:{title}\"\n",
    "    \n",
    "    # We only update Redis if the key is already there. \n",
    "    # If it's not there, Cache-Aside will handle it on the next read.\n",
    "    if r.exists(cache_key):\n",
    "        r.hset(cache_key, key=\"rating\", value=new_rating)\n",
    "        # Reset TTL on update so it stays fresh\n",
    "        r.expire(cache_key, 60)\n",
    "        print(\"Updated MongoDB and Refreshed Redis Cache.\")\n",
    "    else:\n",
    "        print(\"Updated MongoDB only (Key was not in cache).\")\n",
    "\n",
    "print(\" Caching Logic Implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39ed8133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Analytics Logic Implemented.\n"
     ]
    }
   ],
   "source": [
    "def record_view(title):\n",
    "    \"\"\"\n",
    "    Tracks how many times a movie is viewed using Redis Sorted Sets.\n",
    "    Score = View Count\n",
    "    Member = Movie Title\n",
    "    \"\"\"\n",
    "    # ZINCRBY increments the score of the member by 1\n",
    "    r.zincrby(\"leaderboard:views\", 1, title)\n",
    "\n",
    "def get_trending_movies(top_n=5):\n",
    "    \"\"\"\n",
    "    Retrieves the top N movies with highest scores (views).\n",
    "    \"\"\"\n",
    "    # ZREVRANGE gets the range sorted from high to low\n",
    "    return r.zrevrange(\"leaderboard:views\", 0, top_n-1, withscores=True)\n",
    "\n",
    "print(\" Analytics Logic Implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2efcf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fetching test data from MongoDB...\n",
      " Starting Benchmark (500 requests)...\n",
      "Benchmark Complete.\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare Test Data\n",
    "# We grab 100 random titles from Mongo to simulate user queries\n",
    "print(\" Fetching test data from MongoDB...\")\n",
    "all_movies = list(movies_col.find({}, {\"title\": 1, \"_id\": 0}).limit(100))\n",
    "movie_titles = [m['title'] for m in all_movies]\n",
    "\n",
    "# Define \"Hot\" keys (20% of movies that get 80% of traffic)\n",
    "hot_movies = movie_titles[:20] \n",
    "\n",
    "# 2. Run Simulation\n",
    "TOTAL_REQUESTS = 500\n",
    "results = []\n",
    "\n",
    "print(f\" Starting Benchmark ({TOTAL_REQUESTS} requests)...\")\n",
    "\n",
    "start_time_global = time.time()\n",
    "\n",
    "for i in range(TOTAL_REQUESTS):\n",
    "    # Simulate Pareto distribution (80/20 rule)\n",
    "    if random.random() < 0.8:\n",
    "        target = random.choice(hot_movies)\n",
    "    else:\n",
    "        target = random.choice(movie_titles)\n",
    "    \n",
    "    # Measure Latency\n",
    "    req_start = time.time()\n",
    "    data, status = get_movie(target)\n",
    "    req_end = time.time()\n",
    "    \n",
    "    # Track Analytics\n",
    "    if status != \"NOT FOUND\":\n",
    "        record_view(target)\n",
    "    \n",
    "    latency_ms = (req_end - req_start) * 1000\n",
    "    results.append({\"status\": status, \"latency\": latency_ms})\n",
    "\n",
    "print(\"Benchmark Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88bc0204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      " FINAL PROJECT REPORT\n",
      "========================================\n",
      "\n",
      "1. CACHE EFFICIENCY\n",
      "   Total Requests: 500\n",
      "   Hits (Redis):   426\n",
      "   Misses (Mongo): 74\n",
      "    Hit Ratio:   85.20%\n",
      "\n",
      "2. PERFORMANCE GAINS\n",
      "   Avg Time (Cache HIT):  0.0744 ms \n",
      "   Avg Time (Cache MISS): 41.5272 ms \n",
      "    Speedup Factor:     558.4x Faster\n",
      "\n",
      "3. POPULARITY LEADERBOARD (Redis Sorted Sets)\n",
      "   #1: Salomè (32 views)\n",
      "   #2: High and Dizzy (28 views)\n",
      "   #3: The Perils of Pauline (27 views)\n",
      "   #4: From Hand to Mouth (25 views)\n",
      "   #5: Traffic in Souls (23 views)\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# 1. Hit/Miss Ratio\n",
    "hits = len(df[df['status'] == 'HIT'])\n",
    "misses = len(df[df['status'] == 'MISS'])\n",
    "total = hits + misses\n",
    "hit_ratio = (hits / total) * 100\n",
    "\n",
    "# 2. Latency Analysis\n",
    "avg_hit_time = df[df['status'] == 'HIT']['latency'].mean()\n",
    "avg_miss_time = df[df['status'] == 'MISS']['latency'].mean()\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\" FINAL PROJECT REPORT\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(f\"\\n1. CACHE EFFICIENCY\")\n",
    "print(f\"   Total Requests: {TOTAL_REQUESTS}\")\n",
    "print(f\"   Hits (Redis):   {hits}\")\n",
    "print(f\"   Misses (Mongo): {misses}\")\n",
    "print(f\"    Hit Ratio:   {hit_ratio:.2f}%\")\n",
    "\n",
    "print(f\"\\n2. PERFORMANCE GAINS\")\n",
    "print(f\"   Avg Time (Cache HIT):  {avg_hit_time:.4f} ms \")\n",
    "print(f\"   Avg Time (Cache MISS): {avg_miss_time:.4f} ms \")\n",
    "print(f\"    Speedup Factor:     {avg_miss_time / avg_hit_time:.1f}x Faster\")\n",
    "\n",
    "print(f\"\\n3. POPULARITY LEADERBOARD (Redis Sorted Sets)\")\n",
    "trending = get_trending_movies(5)\n",
    "for rank, (name, views) in enumerate(trending, 1):\n",
    "    print(f\"   #{rank}: {name} ({int(views)} views)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef3e0e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---  Testing Write-Through Strategy ---\n",
      "Current Rating: 3.6\n",
      " Updating rating for 'The Saphead' to 1.0...\n",
      "Updated MongoDB and Refreshed Redis Cache.\n",
      "New Rating:     1.0 (Source: HIT)\n",
      " SUCCESS: Data is consistent across DB and Cache.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n---  Testing Write-Through Strategy ---\")\n",
    "\n",
    "# Pick a movie\n",
    "test_movie = hot_movies[0]\n",
    "\n",
    "# 1. Read current rating (Should be a HIT now)\n",
    "data, _ = get_movie(test_movie)\n",
    "print(f\"Current Rating: {data['rating']}\")\n",
    "\n",
    "# 2. Update rating\n",
    "new_rating = round(random.uniform(1.0, 10.0), 1)\n",
    "update_movie_rating(test_movie, new_rating)\n",
    "\n",
    "# 3. Read again (Should immediately reflect new rating)\n",
    "data_new, status = get_movie(test_movie)\n",
    "print(f\"New Rating:     {data_new['rating']} (Source: {status})\")\n",
    "\n",
    "if data_new['rating'] == str(new_rating):\n",
    "    print(\" SUCCESS: Data is consistent across DB and Cache.\")\n",
    "else:\n",
    "    print(\"FAILURE: Data mismatch.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
